{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Train_Faster-RCNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyGLTWptIfKU"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdKlMhgXIvnG"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRULS0vwIvye"
      },
      "source": [
        "import json\n",
        "from PIL import Image\n",
        "import torch\n",
        "from pycocotools import mask as coco_mask\n",
        "from pycocotools.coco import COCO\n",
        "import os\n",
        "import numpy as np\n",
        "class myDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, img_dir, ann_path, transforms=None):\n",
        "        self.img_dir = img_dir\n",
        "        self.transforms = transforms\n",
        "        self.imgs = list(sorted(os.listdir( img_dir )))\n",
        "        with open(ann_path,'r') as f:\n",
        "            self.annotations = json.load(f)\n",
        "         \n",
        "    def __len__(self):\n",
        "        return len(self.annotations[\"images\"])\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_dir = self.img_dir\n",
        "        annotations = self.annotations\n",
        "        transforms = self.transforms\n",
        "\n",
        "        file_name = annotations[\"images\"][index]['file_name']\n",
        "        img_path = os.path.join(img_dir, file_name)\n",
        "\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        height = annotations[\"images\"][index]['height']\n",
        "        width = annotations[\"images\"][index]['width']\n",
        "        annotations =[(ann) for ann in self.annotations[\"annotations\"] if ann['image_id'] == index]\n",
        "        target = {}\n",
        "        area = []\n",
        "        labels = []\n",
        "        masks = []\n",
        "        boxes = []\n",
        "        for annotation in annotations:\n",
        " \n",
        "            area.append(annotation[\"area\"])\n",
        "            labels.append(annotation[\"category_id\"])\n",
        "            xmin = annotation['bbox'][0]\n",
        "            xmax = annotation['bbox'][0]+annotation['bbox'][2]\n",
        "            ymin = annotation['bbox'][1]\n",
        "            ymax = annotation['bbox'][1]+annotation['bbox'][3]\n",
        "            boxes.append([xmin, ymin, xmax, ymax])\n",
        "            # segmentation = annotation[\"segmentation\"]\n",
        "\n",
        "            # rles = coco_mask.frPyObjects(segmentation, height, width)\n",
        "            # mask = coco_mask.decode(rles)\n",
        "            # if len(mask.shape) < 3:\n",
        "            #     mask = mask[..., None]\n",
        "            # mask = torch.as_tensor(mask, dtype=torch.uint8)\n",
        "            # mask = mask.any(dim=2)\n",
        "            # masks.append(mask.numpy())\n",
        "            \n",
        "            # pos = np.where(mask.numpy())\n",
        "            # xmin = np.min(pos[1])\n",
        "            # xmax = np.max(pos[1])\n",
        "            # ymin = np.min(pos[0])\n",
        "            # ymax = np.max(pos[0])\n",
        "            #boxes.append([xmin, ymin, xmax, ymax])\n",
        "\n",
        "\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = torch.as_tensor(labels)\n",
        "        #target[\"masks\"] = torch.as_tensor(masks)\n",
        "        target[\"image_id\"] = torch.tensor([index])\n",
        "        target[\"area\"] = torch.tensor(area)\n",
        "        target[\"iscrowd\"] = torch.zeros(len(annotations), dtype=torch.int64)\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            img, target = self.transforms(img, target)\n",
        "\n",
        "        return img, target"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rLEzOTHIwnk"
      },
      "source": [
        "dataset = myDataset('/content/train',\"/content/train/_annotations.coco.json\")#,get_transform(train=True))\n",
        "dataset[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gtK864SI0ts"
      },
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
        "\n",
        "      \n",
        "def get_model(num_classes):\n",
        "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        " \n",
        "    # replace the classifier with a new one, that has\n",
        "    # num_classes which is user-defined\n",
        "    # get number of input features for the classifier\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    # replace the pre-trained head with a new one\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "    return model"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AET7QUtAJCzo"
      },
      "source": [
        "%%shell\n",
        "\n",
        "# Download TorchVision repo to use some files from\n",
        "# references/detection\n",
        "git clone https://github.com/pytorch/vision.git\n",
        "cd vision\n",
        "git checkout v0.3.0\n",
        "\n",
        "cp references/detection/utils.py ../\n",
        "cp references/detection/transforms.py ../\n",
        "cp references/detection/coco_eval.py ../\n",
        "cp references/detection/engine.py ../\n",
        "cp references/detection/coco_utils.py ../"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OQ7iumqJGZC"
      },
      "source": [
        "import transforms as T\n",
        "def get_transform(train):\n",
        "    transforms = []\n",
        "    transforms.append(T.ToTensor())\n",
        "    return T.Compose(transforms)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cak6hrsZJJzY"
      },
      "source": [
        "import utils\n",
        "dataset = myDataset('/content/train',\"/content/train/_annotations.coco.json\", get_transform(train=True))\n",
        "dataset_test= myDataset('/content/test',\"/content/test/_annotations.coco.json\", get_transform(train=False))\n",
        "\n",
        "# split the dataset in train and test set\n",
        "# torch.manual_seed(1)\n",
        "# indices = torch.randperm(len(dataset)).tolist()\n",
        "# dataset = torch.utils.data.Subset(dataset, indices[:-5])\n",
        "# dataset_test = torch.utils.data.Subset(dataset_test, indices[-5:])\n",
        "\n",
        "# define training and validation data loaders\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        "    dataset, batch_size=2, shuffle=True, num_workers=4,\n",
        "    collate_fn=utils.collate_fn)\n",
        "\n",
        "data_loader_test = torch.utils.data.DataLoader(\n",
        "    dataset_test, batch_size=1, shuffle=False, num_workers=4,\n",
        "    collate_fn=utils.collate_fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HjUOpxmJNrI"
      },
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "# our dataset has two classes only - background and person\n",
        "num_classes = 4\n",
        "\n",
        "# get the model using our helper function\n",
        "model = get_instance_segmentation_model(num_classes)\n",
        "# move model to the right device\n",
        "model.to(device)\n",
        "\n",
        "# construct an optimizer\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=0.005,\n",
        "                            momentum=0.9, weight_decay=0.0005)\n",
        "\n",
        "# and a learning rate scheduler which decreases the learning rate by\n",
        "# 10x every 3 epochs\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
        "                                               step_size=5,\n",
        "                                               gamma=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eX7Gvfp_JReg"
      },
      "source": [
        "from engine import train_one_epoch, evaluate\n",
        "# let's train it for 10 epochs\n",
        "num_epochs = 25\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # train for one epoch, printing every 10 iterations\n",
        "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
        "    # update the learning rate\n",
        "    lr_scheduler.step()\n",
        "    # evaluate on the test dataset\n",
        "    evaluate(model, data_loader_test, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zL69n-WvJVLj"
      },
      "source": [
        "torch.save(model, \"/content/gdrive/MyDrive/Devoworm/Train/entire_model.pth\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# pick one image from the test set\n",
        "img, _ = dataset_test[1]\n",
        "# put the model in evaluation mode\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    prediction = model([img.to(device)])\n",
        "prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "indc = [(ind) for ind,obj in enumerate(prediction[0]['scores']) if obj>0.93]\n",
        "print(indc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "# Reading an image in default mode\n",
        "image = img.mul(255).permute(1, 2, 0).byte().numpy()\n",
        "\n",
        "# Window name in which image is displayed\n",
        "window_name = 'Image'\n",
        "\n",
        "# Blue color in BGR\n",
        "color = (255, 0, 0)\n",
        "\n",
        "# Line thickness of 2 px\n",
        "thickness = 2\n",
        "# for box in prediction[0]['boxes'][indc].cpu().numpy():\n",
        "#   start_point = (box[0], box[1])\n",
        "#   end_point = (box[2], box[3])\n",
        "  # image = cv2.rectangle(image, start_point, end_point, color, thickness)\n",
        "# Using cv2.rectangle() method\n",
        "# Draw a rectangle with blue line borders of thickness of 2 px\n",
        "#image = cv2.rectangle(image, start_point, end_point, color, thickness)\n",
        "\n",
        "# Displaying the image\n",
        "#cv2_imshow(image)\n",
        "#plt.imshow(image)\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(15, 15))\n",
        "\n",
        "# Display the image\n",
        "ax.imshow(image)\n",
        "\n",
        "for box in prediction[0]['boxes'][indc].cpu().numpy():\n",
        "  ax.add_patch(patches.Rectangle((box[0], box[1]), box[2]-box[0], box[3]-box[1], linewidth=2, edgecolor='r', facecolor='none'))\n",
        "\n",
        "plt.show()"
      ]
    }
  ]
}